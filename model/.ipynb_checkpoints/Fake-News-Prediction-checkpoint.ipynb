{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800b7bee",
   "metadata": {},
   "source": [
    "*[Importing](https://www.kaggle.com/competitions/fake-news/data) the data and dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99fd144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nandan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stopwords=nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd2dc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re #regex\n",
    "# natural language tool kit\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f440c",
   "metadata": {},
   "source": [
    "Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d8d1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title           author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...    Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...  Daniel J. Flynn   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the dataset into data frame\n",
    "news_df=pd.read_csv(\"./train.csv\")\n",
    "news_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a392db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1bed461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050eec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=news_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a850b698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18285, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e87cb9",
   "metadata": {},
   "source": [
    "as out of 20k we are getting back 18k rows\n",
    "it should be good enough\n",
    "\n",
    "if many rows were missing them we could use methods like imputation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc16c6c6",
   "metadata": {},
   "source": [
    "#### we shall consider the title and author of detecting the news is fake or not \n",
    "we could have used the text i.e is the body of the news but the body could be really long that would dramtically increase the training time \n",
    "also its the title of the article which attracts the viewers to click on that fake news after that they read the content of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cf70d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title         author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...  Darrell Lucus   \n",
       "\n",
       "                                                text  label  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "\n",
       "                                             content  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging the author name and title\n",
    "\n",
    "news_df['content']=news_df['title']+\" \"+news_df['author']+news_df['text']\n",
    "news_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3519a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    House Dem Aide: We Didn’t Even See Comey’s Let...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seperating the independant variables and op variable\n",
    "X=news_df['content']\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df48148d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=news_df[['label']]\n",
    "y.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d8b29",
   "metadata": {},
   "source": [
    "Stemming:\n",
    "\n",
    "Stemming is the process of reducing a word to its Root word\n",
    "\n",
    "example: actor, actress, acting --> act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b71ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35b5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content) # we just want words without special characters and digits\n",
    "    stemmed_content = stemmed_content.lower() # make it  all lower\n",
    "    stemmed_content = stemmed_content.split() # covert string to arr for the below line\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    # remove stop words like is and at these are insignificant words that are used for sentense formatation\n",
    "    # they are not good keywords\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3494fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75acd4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    hous dem aid even see comey letter jason chaff...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(1) # can notice all stop words are gone\n",
    "# no special characters no nuumbers\n",
    "# and also we have reduced every word to its base word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bed385",
   "metadata": {},
   "source": [
    "#### TfidVectorizer is very similar to that of CountVectorizer \n",
    "(bag of words approach to convert the textual data to vectors)\n",
    "Bag-of-Words:\n",
    "##### The bag-of-words model converts text into fixed-length vectors by counting how many times each word appears.\n",
    "\n",
    "\n",
    "##### Term Frequency Inverse Document Frequency (TFIDF) :\n",
    "TFIDF works by proportionally increasing the number of times a word appears in the document but is counterbalanced by the number of documents in which it is present. Hence, words like ‘this’, ’are’ etc., that are commonly present in all the documents are not given a very high rank. However, a word that is present too many times in a few of the documents will be given a higher rank as it might be indicative of the context of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34156005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 112261)\t0.049266288431976044\n",
      "  (0, 112203)\t0.018822028602683175\n",
      "  (0, 111235)\t0.044332109417032826\n",
      "  (0, 111231)\t0.0952049285944191\n",
      "  (0, 111188)\t0.037590717156198834\n",
      "  (0, 111151)\t0.011177780002938412\n",
      "  (0, 110412)\t0.017156396746647547\n",
      "  (0, 109587)\t0.017030258214578126\n",
      "  (0, 109483)\t0.012515568355693731\n",
      "  (0, 109390)\t0.029807510521693708\n",
      "  (0, 109314)\t0.012677958954410361\n",
      "  (0, 109094)\t0.01164029909301929\n",
      "  (0, 108202)\t0.025650088468419645\n",
      "  (0, 108168)\t0.03154160796361522\n",
      "  (0, 107174)\t0.0216800693516796\n",
      "  (0, 105849)\t0.06552976152279262\n",
      "  (0, 105187)\t0.033407039293133704\n",
      "  (0, 104931)\t0.016231190325023088\n",
      "  (0, 104181)\t0.03819403882577124\n",
      "  (0, 103565)\t0.010943506714122472\n",
      "  (0, 103556)\t0.042297456119198974\n",
      "  (0, 103503)\t0.13826067644470952\n",
      "  (0, 103346)\t0.07074511938458296\n",
      "  (0, 102115)\t0.03965402110564886\n",
      "  (0, 101563)\t0.027042557452073717\n",
      "  :\t:\n",
      "  (18284, 7564)\t0.010375309794254967\n",
      "  (18284, 7250)\t0.027544993804815406\n",
      "  (18284, 6942)\t0.03863683979084329\n",
      "  (18284, 6904)\t0.025146951421624444\n",
      "  (18284, 6633)\t0.04242356041848506\n",
      "  (18284, 6199)\t0.030761236525678855\n",
      "  (18284, 5995)\t0.04560722304305648\n",
      "  (18284, 5656)\t0.0234237663459933\n",
      "  (18284, 5373)\t0.04458622802388594\n",
      "  (18284, 3335)\t0.03482181606455688\n",
      "  (18284, 3096)\t0.048312810509707975\n",
      "  (18284, 3014)\t0.014999781793215355\n",
      "  (18284, 2775)\t0.04728684053624693\n",
      "  (18284, 2354)\t0.08873518425264454\n",
      "  (18284, 2075)\t0.022785065927619943\n",
      "  (18284, 2073)\t0.02899439320803489\n",
      "  (18284, 2031)\t0.03284957582054881\n",
      "  (18284, 1974)\t0.03313602191200964\n",
      "  (18284, 1007)\t0.01859910418685415\n",
      "  (18284, 862)\t0.03771932061786278\n",
      "  (18284, 835)\t0.013494037782287048\n",
      "  (18284, 780)\t0.04135568995720008\n",
      "  (18284, 541)\t0.02415004264122745\n",
      "  (18284, 307)\t0.01509237981451121\n",
      "  (18284, 289)\t0.01869233168524983\n"
     ]
    }
   ],
   "source": [
    "# converting the textual data to numberical data\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "#Docstring:     \n",
    "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "#Equivalent to :class:`CountVectorizer` followed by\n",
    "#:class:`TfidfTransformer`.\n",
    "vectorizer.fit(X)\n",
    "X=vectorizer.transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bb7248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2)\n",
    "\n",
    "# staritfy is used to maintain the balance or ration of y being 1 and 0\n",
    "\n",
    "#Some classification problems do not have a balanced number of examples for each class label. \n",
    "#As such, it is desirable to split the dataset into train and test sets in a way that preserves \n",
    "#the same proportions of examples in each class as observed in the original dataset. \n",
    "#This is called a stratified train-test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "447a254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticReg= LogisticRegression()\n",
    "# signmoid func -> S curve \n",
    "#y=1/1+e^-z \n",
    "# z->st line -> z=wX+b\n",
    "# X->content-> input feature\n",
    "# y->predict prob\n",
    "# w->weigts -> imp of feature\n",
    "# b->bias ->intercept\n",
    "# rule of thumb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d8697c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nandan/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticReg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8dffef97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.954334153677878"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticReg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5bc845b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9782608695652174"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticReg.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65fca8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 111553)\t0.11030054595900042\n",
      "  (0, 111151)\t0.043501355810040866\n",
      "  (0, 106085)\t0.1747448147589662\n",
      "  (0, 105708)\t0.2406475856302974\n",
      "  (0, 96268)\t0.10030923655372144\n",
      "  (0, 96113)\t0.18665258944337157\n",
      "  (0, 94349)\t0.07448390572980304\n",
      "  (0, 86572)\t0.29260604560252457\n",
      "  (0, 83307)\t0.0677396613533566\n",
      "  (0, 82689)\t0.3336028280098954\n",
      "  (0, 80555)\t0.46251749665328545\n",
      "  (0, 79988)\t0.3518279521934827\n",
      "  (0, 79032)\t0.2528809127228766\n",
      "  (0, 78307)\t0.05547820564183474\n",
      "  (0, 75277)\t0.16186077257701958\n",
      "  (0, 75115)\t0.16042258049417304\n",
      "  (0, 72764)\t0.22583494818851174\n",
      "  (0, 71516)\t0.09898262795606953\n",
      "  (0, 71327)\t0.05927155656588732\n",
      "  (0, 69106)\t0.04713858926906782\n",
      "  (0, 69051)\t0.06851801655033608\n",
      "  (0, 56723)\t0.10667149463407405\n",
      "  (0, 45550)\t0.08876733318129006\n",
      "  (0, 45548)\t0.13412499758585028\n",
      "  (0, 45101)\t0.06081920989581472\n",
      "  (0, 44226)\t0.05780367794798305\n",
      "  (0, 42297)\t0.058361317658818426\n",
      "  (0, 34424)\t0.042287714187449296\n",
      "  (0, 32489)\t0.07140180497002598\n",
      "  (0, 25367)\t0.12951533095546938\n",
      "  (0, 23284)\t0.07583619657067207\n",
      "  (0, 22530)\t0.07763871328055713\n",
      "  (0, 15300)\t0.07670730224994601\n",
      "  (0, 6059)\t0.15980111924091778\n",
      "  (0, 4943)\t0.055662069673126154\n"
     ]
    }
   ],
   "source": [
    "print(x_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e347c",
   "metadata": {},
   "source": [
    "the acuracy score is pretty much the same\n",
    "this means our model doesn't try to over fit the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4a299",
   "metadata": {},
   "source": [
    " ### making a predictive system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82f5b94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "X_new = x_test[1]\n",
    "prediction = logisticReg.predict(X_new)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8a175ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = open(\"fake-news-model.pickle\",\"wb\") # file pointer\n",
    "fp2 = open(\"vectorizer.pickle\",\"wb\") # file pointer\n",
    "pickle.dump(logisticReg,fp)\n",
    "pickle.dump(X,fp2)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c9aa08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_into_numbers(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content) # we just want words without special characters and digits\n",
    "    stemmed_content = stemmed_content.lower() # make it  all lower\n",
    "    stemmed_content = stemmed_content.split() # covert string to arr for the below line\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    # remove stop words like is and at these are insignificant words that are used for sentense formatation\n",
    "    # they are not good keywords\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    #Docstring:     \n",
    "    #Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "    #Equivalent to :class:`CountVectorizer` followed by\n",
    "    #:class:`TfidfTransformer`.\n",
    "    vectorizer.fit(stemmed_content)\n",
    "    X=vectorizer.transform(stemmed_content)\n",
    "    predict = logisticReg.predict(X)\n",
    "    return predict \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b460eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a8a880b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4cda6007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer()\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75381006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
